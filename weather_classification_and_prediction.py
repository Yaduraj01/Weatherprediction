# -*- coding: utf-8 -*-
"""weather classification and prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_RbK5jJCswuvlLzGRUgoEzc-4N3ebsme

The aim is to train a deep learning model that looks at an image of outdoor scenery and predicts the weather condition shown in it.
"""

import tensorflow as tf
print(tf.__version__)
import keras
print(keras.__version__)

import torch
torch.cuda.is_available()

from google.colab import drive

import zipfile

# Replace with the correct path
zip_file_path = '/content/drive/MyDrive/Deep learning/weather dataset.zip'
extract_path = '/content/drive/MyDrive/DL dataset'

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extraction complete!")

"""**1) PREPARING LABELS (FEATURES : RAIN, CLOUDY, SHINE, SUNRISE )**"""

import os
import re
import numpy as np
from sklearn.model_selection import train_test_split

#path to dataset conatinaing img
dataset_path = '/content/drive/MyDrive/DL dataset/dataset2'

#list all files
all_files = os.listdir(dataset_path)
print(all_files)
#initialize lists for img paths and labels
img_paths = []
labels = []

#map class labels to numeric values using the following dictionary
class_mapping = {'cloudy': 0, 'rain': 1, 'shine': 2, 'sunrise': 3}

#iterate through all files and extract the class labels from FILENAME
for file_name in all_files:
    if file_name.endswith('.jpg'):
        label = re.match(r"[a-zA-Z]+", file_name).group()
        if label in class_mapping:
            img_paths.append(os.path.join(dataset_path, file_name))
            labels.append(class_mapping[label])

labels = np.array(labels)

print(img_paths)
print (labels)

from sklearn.model_selection import train_test_split

# Split the data into training and validation sets (80% train, 20% validation)
X_train, X_val, y_train, y_val = train_test_split(img_paths, labels, test_size=0.2, random_state=42)

# Split the training set further into training and test sets (75% train, 25% test)
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42)

import tensorflow as tf

# Set GPU memory growth to avoid TensorFlow taking all the GPU memory at once
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

print("Using GPU: ", physical_devices)

"""**2) LOAD AND PREPROCESS DATA**"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.applications.vgg16 import preprocess_input

#function to load and preprocess images
def load_and_preprocess_images(img_paths, traget_size =(224,224)):
  img_list = []

  for img_path in img_paths:
    #load the img
    img = image.load_img(img_path, target_size=(224,224))

    #convert img to a NUMPY array
    img_array = image.img_to_array(img)

    #normalize the image (scaling pixel values between 0 and 1)
    img_array = img_array / 255.0

    img_list.append(img_array)

  return np.array(img_list) #(number of img, heightm width, channels)

#preprocess training, validation and tests img
X_train_processed = load_and_preprocess_images(X_train)
X_val_processed = load_and_preprocess_images(X_val)
X_test_processed = load_and_preprocess_images(X_test)

# Convert labels to categorical (one-hot encoding) for multi-class classification
y_train_processed = to_categorical(y_train, num_classes=len(class_mapping))
y_val_processed = to_categorical(y_val, num_classes=len(class_mapping))
y_test_processed = to_categorical(y_test, num_classes=len(class_mapping))

# Check the shape of the processed images
print(f"Training images shape: {X_train_processed.shape}")
print(f"Validation images shape: {X_val_processed.shape}")
print(f"Test images shape: {X_test_processed.shape}")

print(y_train_processed)

"""**3) MODEL BUILDING (CNN MODEL)**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

model = Sequential()

#add conv. layer
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))  # input shape (height, width, channels)
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Dense(128, activation='relu'))

#flatten the feature maps to prepare fully connected layes
model.add(Flatten())

#add fully connected layers
model.add(Dropout(0.5))
model.add(Dense(len(class_mapping), activation='softmax'))

#compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

print(X_train_processed.shape)  # Should output (num_samples, 224, 224, 3)
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print(type(X_train_processed), X_train_processed.dtype)  # Should be numpy array and float32
print(y_train_processed.shape)  # Ensure this matches the number of classes and the number of samples

# Train the model
history = model.fit(X_train_processed, y_train_processed, epochs=10, batch_size=18, validation_data=(X_val_processed, y_val_processed))

"""**EVALUATION**

"""

# Evaluate on the validation set
val_loss, val_accuracy = model.evaluate(X_val_processed, y_val_processed)
print(f"Validation Loss: {val_loss}")
print(f"Validation Accuracy: {val_accuracy}")

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_processed, y_test_processed)

# Print the test results
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

from sklearn.metrics import classification_report, confusion_matrix

# Predict the class labels for validation data
y_pred = model.predict(X_val_processed)
y_pred_classes = np.argmax(y_pred, axis=1)

# Print confusion matrix
print(confusion_matrix(np.argmax(y_val_processed, axis=1), y_pred_classes))

# Print classification report
print(classification_report(np.argmax(y_val_processed, axis=1), y_pred_classes))

import matplotlib.pyplot as plt

# Plot accuracy
plt.plot(history.history['accuracy'], label='train accuracy')
plt.plot(history.history['val_accuracy'], label='val accuracy')
plt.legend()
plt.title("Accuracy vs Epochs")
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.title("Loss vs Epochs")
plt.show()

model.save('/content/drive/MyDrive/Deep learning/MyModels/weathermodel.keras')  # Save as Keras format

